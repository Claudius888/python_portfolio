{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed3d240",
   "metadata": {},
   "source": [
    "## Fine Tuning spaCy NER Model to identify food products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb4a4e",
   "metadata": {},
   "source": [
    "I'm building an app that recognizes entities when having a conversation. \n",
    "To do this, I'll be making use of spaCy for natural language processing (NLP).\n",
    "In this notebook, we'll be training spaCy to identify FOOD entities from a body of text - a task known as named-entity recognition (NER). \n",
    "If all goes well, we should be able to identify the foods from the following sentences:\n",
    "\n",
    "- Can I have a hamburger and fries  \n",
    "- I want a taco with tabasco and nachos\n",
    "- I would like to have rice  with curry and potatoes\n",
    "\n",
    "spaCy has a NER accuracy of 85.85%, so something in that range would be nice for our FOOD entities.\n",
    "\n",
    "Approach\n",
    "We'll use the following approach:\n",
    "\n",
    "- Generate sentences with FOOD entities.\n",
    "- Generate sentences with existing spaCy entities to avoid the catastrophic forgetting problem.\n",
    "- Train spaCy NER with the existing entities and the custom FOOD entities. Stir until good enough.\n",
    "\n",
    "Results\n",
    "Category\t     |Results\n",
    "-----------------|----------\n",
    "FOOD Entities\t |97.47%\n",
    "Existing Entities|78.17%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "689c648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_lg\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e587c",
   "metadata": {},
   "source": [
    "### Gathering Food Data\n",
    "We'll be using food data from the [USDA's Branded Food's dataset.](https://fdc.nal.usda.gov/download-datasets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c4fd4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fdc_id</th>\n",
       "      <th>data_type</th>\n",
       "      <th>description</th>\n",
       "      <th>food_category_id</th>\n",
       "      <th>publication_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1105904</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>WESSON Vegetable Oil 1 GAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1105905</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>SWANSON BROTH BEEF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1105906</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>CAMPBELL'S SLOW KETTLE SOUP CLAM CHOWDER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1105907</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>CAMPBELL'S SLOW KETTLE SOUP CHEESE BROCCOLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1105908</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>SWANSON BROTH CHICKEN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fdc_id     data_type                                  description  \\\n",
       "0  1105904  branded_food                   WESSON Vegetable Oil 1 GAL   \n",
       "1  1105905  branded_food                           SWANSON BROTH BEEF   \n",
       "2  1105906  branded_food     CAMPBELL'S SLOW KETTLE SOUP CLAM CHOWDER   \n",
       "3  1105907  branded_food  CAMPBELL'S SLOW KETTLE SOUP CHEESE BROCCOLI   \n",
       "4  1105908  branded_food                        SWANSON BROTH CHICKEN   \n",
       "\n",
       "   food_category_id publication_date  \n",
       "0               NaN       2020-11-13  \n",
       "1               NaN       2020-11-13  \n",
       "2               NaN       2020-11-13  \n",
       "3               NaN       2020-11-13  \n",
       "4               NaN       2020-11-13  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the food csv file\n",
    "food_df = pd.read_csv(\"./food_data/food.csv\")\n",
    "\n",
    "# print row and column information\n",
    "food_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a7250",
   "metadata": {},
   "source": [
    "### Cause there are more rows, so we will:\n",
    "\n",
    "- Remove foods with special characters (+,&, !, etc.).\n",
    "- Filter out foods containing more than 3 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab289e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41009"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove foods with special characters, lowercase and extract results from \"description\" column\n",
    "foods = food_df[food_df[\"description\"].str.contains(\"[^a-zA-Z ]\") == False][\"description\"].apply(lambda food: food.lower())\n",
    "\n",
    "# filter out foods with more than 3 words, drop any duplicates\n",
    "foods = foods[foods.str.split().apply(len) <= 3].drop_duplicates()\n",
    "\n",
    "# print the remaining size\n",
    "foods.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc9d2790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find one-worded, two-worded and three-worded foods\n",
    "one_worded_foods = foods[foods.str.split().apply(len) == 1]\n",
    "two_worded_foods = foods[foods.str.split().apply(len) == 2]\n",
    "three_worded_foods = foods[foods.str.split().apply(len) == 3]\n",
    "\n",
    "# create a bar plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar([1, 2, 3], [one_worded_foods.size, two_worded_foods.size, three_worded_foods.size])\n",
    "\n",
    "# label the x-axis instances\n",
    "ax.set_xticks([1, 2, 3])\n",
    "ax.set_xticklabels([\"one\", \"two\", \"three\"])\n",
    "\n",
    "# set the title and the xy-axis labels\n",
    "plt.title(\"Number of Words in Food Entities\")\n",
    "plt.xlabel(\"Number of Words\")\n",
    "plt.ylabel(\"Food Entities\")\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f4a26",
   "metadata": {},
   "source": [
    "Because the majority of our food entities are multi-worded, spaCy would develop a bias for multi-worded foods. If we look back to our example of grilled cheese, it's not a big deal if spaCy identifies cheese instead of grilled cheese. It is a big deal if spaCy fails to identify cheese at all.\n",
    "\n",
    "So let's filter the dataset further, such that 45% are one-worded foods, 30% are two-worded foods, and 25% are three-worded foods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e983243e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-worded food entities: 1378\n",
      "2-worded food entities: 919\n",
      "3-worded food entities: 766\n"
     ]
    }
   ],
   "source": [
    "# total number of foods\n",
    "total_num_foods = round(one_worded_foods.size / 45 * 100)\n",
    "\n",
    "# shuffle the 2-worded and 3-worded foods since we'll be slicing them\n",
    "two_worded_foods = two_worded_foods.sample(frac=1)\n",
    "three_worded_foods = three_worded_foods.sample(frac=1)\n",
    "\n",
    "# append the foods together \n",
    "foods = one_worded_foods.append(two_worded_foods[:round(total_num_foods * 0.30)]).append(three_worded_foods[:round(total_num_foods * 0.25)])\n",
    "\n",
    "# print the resulting sizes\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}-worded food entities:\", foods[foods.str.split().apply(len) == i + 1].size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d83b9",
   "metadata": {},
   "source": [
    "## Train test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa43c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_templates = [\n",
    "    \"I want a {}\",\n",
    "    \"I would like to have a {}\",\n",
    "    \"Imma need a {}\",\n",
    "    \"Can I have a {}\",\n",
    "    \"I want a {} with {}\",\n",
    "    \"I would like to have a {} with {}\",\n",
    "    \"Imma need a {} with {}\",\n",
    "    \"Can I have a {} with {}\",\n",
    "    \"I want a {}, {}\",\n",
    "    \"I would like to have a {}, {}\",\n",
    "    \"Imma need a {}, {}\",\n",
    "    \"Can I have a {},  {}\",\n",
    "    \"I want a {} with {} and {}\",\n",
    "    \"I would like to have a {} with {} and {}\",\n",
    "    \"Imma need a {} with {} and {}\",\n",
    "    \"Can I have a {} with {} and {}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3a783fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries to store the generated food combinations. Do note that one_food != one_worded_food. one_food == \"barbecue sauce\", one_worded_food == \"sauce\"\n",
    "TRAIN_FOOD_DATA = {\n",
    "    \"one_food\": [],\n",
    "    \"two_foods\": [],\n",
    "    \"three_foods\": []\n",
    "}\n",
    "\n",
    "TEST_FOOD_DATA = {\n",
    "    \"one_food\": [],\n",
    "    \"two_foods\": [],\n",
    "    \"three_foods\": []\n",
    "}\n",
    "\n",
    "# one_food, two_food, and three_food combinations will be limited to 167 sentences\n",
    "FOOD_SENTENCE_LIMIT = 167\n",
    "\n",
    "# helper function for deciding what dictionary and subsequent array to append the food sentence on to\n",
    "def get_food_data(count):\n",
    "    return {\n",
    "        1: TRAIN_FOOD_DATA[\"one_food\"] if len(TRAIN_FOOD_DATA[\"one_food\"]) < FOOD_SENTENCE_LIMIT else TEST_FOOD_DATA[\"one_food\"],\n",
    "        2: TRAIN_FOOD_DATA[\"two_foods\"] if len(TRAIN_FOOD_DATA[\"two_foods\"]) < FOOD_SENTENCE_LIMIT else TEST_FOOD_DATA[\"two_foods\"],\n",
    "        3: TRAIN_FOOD_DATA[\"three_foods\"] if len(TRAIN_FOOD_DATA[\"three_foods\"]) < FOOD_SENTENCE_LIMIT else TEST_FOOD_DATA[\"three_foods\"],\n",
    "    }[count]\n",
    "\n",
    "# the pattern to replace from the template sentences\n",
    "pattern_to_replace = \"{}\"\n",
    "\n",
    "# shuffle the data before starting\n",
    "foods = foods.sample(frac=1)\n",
    "\n",
    "# the count that helps us decide when to break from the for loop\n",
    "food_entity_count = foods.size - 1\n",
    "\n",
    "# start the while loop, ensure we don't get an index out of bounds error\n",
    "while food_entity_count >= 2:\n",
    "    entities = []\n",
    "\n",
    "    # pick a random food template\n",
    "    sentence = food_templates[random.randint(0, len(food_templates) - 1)]\n",
    "\n",
    "    # find out how many braces \"{}\" need to be replaced in the template\n",
    "    matches = re.findall(pattern_to_replace, sentence)\n",
    "\n",
    "    # for each brace, replace with a food entity from the shuffled food data\n",
    "    for match in matches:\n",
    "        food = foods.iloc[food_entity_count]\n",
    "        food_entity_count -= 1\n",
    "\n",
    "        # replace the pattern, but then find the match of the food entity we just inserted\n",
    "        sentence = sentence.replace(match, food, 1)\n",
    "        match_span = re.search(food, sentence).span()\n",
    "\n",
    "        # use that match to find the index positions of the food entity in the sentence, append\n",
    "        entities.append((match_span[0], match_span[1], \"FOOD\"))\n",
    "\n",
    "    # append the sentence and the position of the entities to the correct dictionary and array\n",
    "    get_food_data(len(matches)).append((sentence, {\"entities\": entities}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8cee8364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167 one_food sentences: ('Can I have a coffee roasters', {'entities': [(13, 28, 'FOOD')]})\n",
      "167 two_foods sentences: ('Imma need a cheesecake platter, kiwi', {'entities': [(12, 30, 'FOOD'), (32, 36, 'FOOD')]})\n",
      "167 three_foods sentences: ('I would like to have a fruit tray with zucchini squash and whole italian sub', {'entities': [(23, 33, 'FOOD'), (39, 54, 'FOOD'), (59, 76, 'FOOD')]})\n"
     ]
    }
   ],
   "source": [
    "# print the number of food sentences, as well as an example sentence\n",
    "for key in TRAIN_FOOD_DATA:\n",
    "    print(\"{} {} sentences: {}\".format(len(TRAIN_FOOD_DATA[key]), key, TRAIN_FOOD_DATA[key][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "422f1bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194 one_food items: ('I want a masterpieces', {'entities': [(9, 21, 'FOOD')]})\n",
      "571 two_foods items: ('I want a cured pork longaniza, general tso chicken', {'entities': [(9, 29, 'FOOD'), (31, 50, 'FOOD')]})\n",
      "241 three_foods items: ('I want a asian yogurt with bauernwurst and classic enriched buns', {'entities': [(9, 21, 'FOOD'), (27, 38, 'FOOD'), (43, 64, 'FOOD')]})\n"
     ]
    }
   ],
   "source": [
    "for key in TEST_FOOD_DATA:\n",
    "    print(\"{} {} items: {}\".format(len(TEST_FOOD_DATA[key]), key, TEST_FOOD_DATA[key][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11cfaf",
   "metadata": {},
   "source": [
    "# Generating Revision Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552aa419",
   "metadata": {},
   "source": [
    "As mentioned in the overview, we also need to generate sentences that contain spaCy entities. This helps us avoid the situation where the NER model is able to identify the FOOD entities, but forgets how to classify entities like DAT or ORG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18321e58",
   "metadata": {},
   "source": [
    "## Preparing the revision data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a25b0180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  In the Washington of 2016, even when the polic...\n",
       "1    Donald Trump has used Twitter  —   his prefe...\n",
       "2    Donald Trump is unabashedly praising Russian...\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...\n",
       "4  From photography, illustration and video, to d..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the revision data (just used a random article dataset from a different course I had taken)\n",
    "npr_df = pd.read_csv(\"npr.csv\")\n",
    "\n",
    "# print row and column information\n",
    "npr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0efa7038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an nlp object as we'll use this to seperate the sentences and identify existing entities\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e320d8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda3\\envs\\pdp\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "revision_texts = []\n",
    "\n",
    "# convert the articles to spacy objects to better identify the sentences. Disabled unneeded components. # takes ~ 4 minutes\n",
    "for doc in nlp.pipe(npr_df[\"Article\"][:6000], batch_size=30, disable=[\"tagger\", \"ner\"]):\n",
    "    for sentence in doc.sents:\n",
    "        if  40 < len(sentence.text) < 80:\n",
    "            # some of the sentences had excessive whitespace in between words, so we're trimming that\n",
    "            revision_texts.append(\" \".join(re.split(\"\\s+\", sentence.text, flags=re.UNICODE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6099f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "revisions = []\n",
    "\n",
    "# Use the existing spaCy model to predict the entities, then append them to revision\n",
    "for doc in nlp.pipe(revision_texts, batch_size=50, disable=[\"tagger\", \"parser\"]):\n",
    "    \n",
    "    # don't append sentences that have no entities\n",
    "    if len(doc.ents) > 0:\n",
    "        revisions.append((doc.text, {\"entities\": [(e.start_char, e.end_char, e.label_) for e in doc.ents]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46ec5bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And in that sense, this year shows little sign of ending on Dec. 31.\n",
      "{'entities': [(19, 28, 'DATE'), (60, 67, 'DATE')]}\n"
     ]
    }
   ],
   "source": [
    "# print an example of the revision sentence\n",
    "print(revisions[0][0])\n",
    "\n",
    "# print an example of the revision data\n",
    "print(revisions[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "049431cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arrays to store the revision data\n",
    "TRAIN_REVISION_DATA = []\n",
    "TEST_REVISION_DATA = []\n",
    "\n",
    "# create dictionaries to keep count of the different entities\n",
    "TRAIN_ENTITY_COUNTER = {}\n",
    "TEST_ENTITY_COUNTER = {}\n",
    "\n",
    "# This will help distribute the entities (i.e. we don't want 1000 PERSON entities, but only 80 ORG entities)\n",
    "REVISION_SENTENCE_SOFT_LIMIT = 100\n",
    "\n",
    "# helper function for incrementing the revision counters\n",
    "def increment_revision_counters(entity_counter, entities):\n",
    "    for entity in entities:\n",
    "        label = entity[2]\n",
    "        if label in entity_counter:\n",
    "            entity_counter[label] += 1\n",
    "        else:\n",
    "            entity_counter[label] = 1\n",
    "\n",
    "random.shuffle(revisions)\n",
    "for revision in revisions:\n",
    "    # get the entities from the revision sentence\n",
    "    entities = revision[1][\"entities\"]\n",
    "\n",
    "    # simple hack to make sure spaCy entities don't get too one-sided\n",
    "    should_append_to_train_counter = 0\n",
    "    for _, _, label in entities:\n",
    "        if label in TRAIN_ENTITY_COUNTER and TRAIN_ENTITY_COUNTER[label] > REVISION_SENTENCE_SOFT_LIMIT:\n",
    "            should_append_to_train_counter -= 1\n",
    "        else:\n",
    "            should_append_to_train_counter += 1\n",
    "\n",
    "    # simple switch for deciding whether to append to train data or test data\n",
    "    if should_append_to_train_counter >= 0:\n",
    "        TRAIN_REVISION_DATA.append(revision)\n",
    "        increment_revision_counters(TRAIN_ENTITY_COUNTER, entities)\n",
    "    else:\n",
    "        TEST_REVISION_DATA.append(revision)\n",
    "        increment_revision_counters(TEST_ENTITY_COUNTER, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d359e982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PERSON': 261,\n",
       " 'GPE': 183,\n",
       " 'CARDINAL': 165,\n",
       " 'ORG': 193,\n",
       " 'NORP': 129,\n",
       " 'TIME': 106,\n",
       " 'DATE': 200,\n",
       " 'ORDINAL': 111,\n",
       " 'PRODUCT': 101,\n",
       " 'QUANTITY': 101,\n",
       " 'MONEY': 104,\n",
       " 'FAC': 101,\n",
       " 'PERCENT': 103,\n",
       " 'LOC': 103,\n",
       " 'EVENT': 101,\n",
       " 'WORK_OF_ART': 103,\n",
       " 'LAW': 70,\n",
       " 'LANGUAGE': 58}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_ENTITY_COUNTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68ca06b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PERSON': 11953,\n",
       " 'ORG': 8200,\n",
       " 'GPE': 5140,\n",
       " 'WORK_OF_ART': 305,\n",
       " 'DATE': 6290,\n",
       " 'ORDINAL': 970,\n",
       " 'FAC': 150,\n",
       " 'TIME': 603,\n",
       " 'CARDINAL': 4648,\n",
       " 'NORP': 2311,\n",
       " 'LOC': 444,\n",
       " 'PRODUCT': 125,\n",
       " 'MONEY': 460,\n",
       " 'QUANTITY': 93,\n",
       " 'PERCENT': 433,\n",
       " 'EVENT': 68,\n",
       " 'LANGUAGE': 16,\n",
       " 'LAW': 10}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_ENTITY_COUNTER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b59cde",
   "metadata": {},
   "source": [
    "# Training the NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e338fd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOOD 501\n",
      "REVISION 1475\n",
      "COMBINED 1976\n"
     ]
    }
   ],
   "source": [
    "# combine the food training data\n",
    "TRAIN_FOOD_DATA_COMBINED = TRAIN_FOOD_DATA[\"one_food\"] + TRAIN_FOOD_DATA[\"two_foods\"] + TRAIN_FOOD_DATA[\"three_foods\"]\n",
    "\n",
    "# print the length of the food training data\n",
    "print(\"FOOD\", len(TRAIN_FOOD_DATA_COMBINED))\n",
    "\n",
    "# print the length of the revision training data\n",
    "print(\"REVISION\", len(TRAIN_REVISION_DATA))\n",
    "\n",
    "# join and print the combined length\n",
    "TRAIN_DATA = TRAIN_REVISION_DATA + TRAIN_FOOD_DATA_COMBINED\n",
    "print(\"COMBINED\", len(TRAIN_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b6e2977",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda3\\envs\\pdp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I would like to have a stewed tomatoes  with cashe...\" with entities \"[(23, 39, 'FOOD'), (45, 61, 'FOOD'), (66, 90, 'FOO...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\miniconda3\\envs\\pdp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I want a orzo with pepper steaks  and shoyu chicke...\" with entities \"[(9, 13, 'FOOD'), (19, 33, 'FOOD'), (38, 57, 'FOOD...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\miniconda3\\envs\\pdp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"Can I have a pistachios  with coolstix and bananas\" with entities \"[(13, 24, 'FOOD'), (30, 38, 'FOOD'), (43, 50, 'FOO...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\miniconda3\\envs\\pdp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I want a biscuits \" with entities \"[(9, 18, 'FOOD')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\miniconda3\\envs\\pdp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I want a organic sweet tea with assorted \" with entities \"[(9, 26, 'FOOD'), (32, 41, 'FOOD')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\miniconda3\\envs\\pdp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I would like to have a biscuits with smoked sausge...\" with entities \"[(23, 31, 'FOOD'), (37, 51, 'FOOD'), (56, 76, 'FOO...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\miniconda3\\envs\\pdp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I want a cavatelli with cocktail mix  and zesty\" with entities \"[(9, 18, 'FOOD'), (24, 37, 'FOOD'), (42, 47, 'FOOD...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\miniconda3\\envs\\pdp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I want a ginger lime mixers with cheese  and garli...\" with entities \"[(9, 27, 'FOOD'), (33, 40, 'FOOD'), (45, 61, 'FOOD...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\miniconda3\\envs\\pdp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I would like to have a marinade  with old fashione...\" with entities \"[(23, 32, 'FOOD'), (38, 60, 'FOOD'), (65, 71, 'FOO...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\miniconda3\\envs\\pdp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I would like to have a parmesan cheese \" with entities \"[(23, 39, 'FOOD')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\miniconda3\\envs\\pdp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I would like to have a maduros ripe plantains  wit...\" with entities \"[(23, 46, 'FOOD'), (52, 72, 'FOOD'), (77, 85, 'FOO...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\miniconda3\\envs\\pdp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"Can I have a dressing \" with entities \"[(13, 22, 'FOOD')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\miniconda3\\envs\\pdp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"Imma need a cookies  with flatbread sesame and cro...\" with entities \"[(12, 20, 'FOOD'), (26, 42, 'FOOD'), (47, 55, 'FOO...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\miniconda3\\envs\\pdp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"Can I have a pita chips  with pumpkin and calamari\" with entities \"[(13, 24, 'FOOD'), (30, 37, 'FOOD'), (42, 50, 'FOO...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses (1/30) {'ner': 1744.4680251873408}\n",
      "Losses (2/30) {'ner': 1303.6553738598473}\n",
      "Losses (3/30) {'ner': 1113.978699317095}\n",
      "Losses (4/30) {'ner': 985.6708172091254}\n",
      "Losses (5/30) {'ner': 957.5738040874099}\n",
      "Losses (6/30) {'ner': 860.3676341461975}\n",
      "Losses (7/30) {'ner': 766.7513463772563}\n",
      "Losses (8/30) {'ner': 722.4744065683285}\n",
      "Losses (9/30) {'ner': 718.4186806882981}\n",
      "Losses (10/30) {'ner': 678.054398603244}\n",
      "Losses (11/30) {'ner': 582.635795398497}\n",
      "Losses (12/30) {'ner': 558.0896443167587}\n",
      "Losses (13/30) {'ner': 575.1688132468013}\n",
      "Losses (14/30) {'ner': 566.3361584044995}\n",
      "Losses (15/30) {'ner': 601.8568268366527}\n",
      "Losses (16/30) {'ner': 526.0792745345688}\n",
      "Losses (17/30) {'ner': 542.5887872962447}\n",
      "Losses (18/30) {'ner': 533.8633648398055}\n",
      "Losses (19/30) {'ner': 420.36137952553617}\n",
      "Losses (20/30) {'ner': 448.5945868204732}\n",
      "Losses (21/30) {'ner': 415.38239799126535}\n",
      "Losses (22/30) {'ner': 368.42756756601943}\n",
      "Losses (23/30) {'ner': 374.2837451244376}\n",
      "Losses (24/30) {'ner': 506.1999263782777}\n",
      "Losses (25/30) {'ner': 359.092147851453}\n",
      "Losses (26/30) {'ner': 361.7119989603997}\n",
      "Losses (27/30) {'ner': 391.7454201510918}\n",
      "Losses (28/30) {'ner': 391.0557072552995}\n",
      "Losses (29/30) {'ner': 367.790967227606}\n",
      "Losses (30/30) {'ner': 334.46200894405564}\n"
     ]
    }
   ],
   "source": [
    "# add NER to the pipeline and the new label\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.add_label(\"FOOD\")\n",
    "\n",
    "# get the names of the components we want to disable during training\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# start the training loop, only training NER\n",
    "epochs = 30\n",
    "optimizer = nlp.resume_training()\n",
    "with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    \n",
    "    # batch up the examples using spaCy's minibatc\n",
    "    for epoch in range(epochs):\n",
    "        examples = TRAIN_DATA\n",
    "        random.shuffle(examples)\n",
    "        batches = minibatch(examples, size=sizes)\n",
    "        losses = {}\n",
    "        \n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            \n",
    "            example = []\n",
    "            # Update the model with iterating each text\n",
    "            for i in range(len(texts)):\n",
    "                doc = nlp.make_doc(texts[i])\n",
    "                example.append(Example.from_dict(doc, annotations[i]))\n",
    "\n",
    "            nlp.update(example, sgd=optimizer, drop=0.35, losses=losses)\n",
    "\n",
    "        print(\"Losses ({}/{})\".format(epoch + 1, epochs), losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb35f32",
   "metadata": {},
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55f81930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display sentence involving original entities\n",
    "spacy.displacy.render(nlp(\"Apple is looking at buying U.K. startup for $1 billion\"), style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae96287a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I would like to have chips.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I want to \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    have chocolate ice cream\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Can I have basmati rice with \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    leaf spinach\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    cheese\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display sentences involving target entity\n",
    "spacy.displacy.render(nlp(\"I would like to have chips.\"), style=\"ent\")\n",
    "spacy.displacy.render(nlp(\"I want to have chocolate ice cream.\"), style=\"ent\")\n",
    "spacy.displacy.render(nlp(\"Can I have basmati rice with leaf spinach and cheese\"), style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b652ee",
   "metadata": {},
   "source": [
    "# Evaluating Food Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6e8b837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to hold our evaluation data\n",
    "food_evaluation = {\n",
    "    \"one_food\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0,\n",
    "    },\n",
    "    \"two_foods\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0\n",
    "    },\n",
    "    \"three_foods\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "word_evaluation = {\n",
    "    \"1_worded_foods\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0\n",
    "    },\n",
    "    \"2_worded_foods\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0\n",
    "    },\n",
    "    \"3_worded_foods\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# loop over data from our test food set (3 keys in total)\n",
    "for key in TEST_FOOD_DATA:\n",
    "    foods = TEST_FOOD_DATA[key]\n",
    "\n",
    "    for food in foods:\n",
    "        # extract the sentence and correct food entities according to our test data\n",
    "        sentence = food[0]\n",
    "        entities = food[1][\"entities\"]\n",
    "\n",
    "        # for each entity, use our updated model to make a prediction on the sentence\n",
    "        for entity in entities:\n",
    "            doc = nlp(sentence)\n",
    "            correct_text = sentence[entity[0]:entity[1]]\n",
    "            n_worded_food =  len(correct_text.split())\n",
    "\n",
    "            # if we find that there's a match for predicted entity and predicted text, increment correct counters\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == entity[2] and ent.text == correct_text:\n",
    "                    food_evaluation[key][\"correct\"] += 1\n",
    "                    if n_worded_food > 0:\n",
    "                        word_evaluation[f\"{n_worded_food}_worded_foods\"][\"correct\"] += 1\n",
    "                    \n",
    "                    # this break is important, ensures that we're not double counting on a correct match\n",
    "                    break\n",
    "            \n",
    "            #  increment total counters after each entity loop\n",
    "            food_evaluation[key][\"total\"] += 1\n",
    "            if n_worded_food > 0:\n",
    "                word_evaluation[f\"{n_worded_food}_worded_foods\"][\"total\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f194171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_worded_foods: 97.01%\n",
      "2_worded_foods: 98.69%\n",
      "3_worded_foods: 96.86%\n",
      "---\n",
      "one_food: 97.94%\n",
      "two_foods: 96.67%\n",
      "three_foods: 98.62%\n",
      "\n",
      "Total: 97.47%\n"
     ]
    }
   ],
   "source": [
    "for key in word_evaluation:\n",
    "    correct = word_evaluation[key][\"correct\"]\n",
    "    total = word_evaluation[key][\"total\"]\n",
    "\n",
    "    print(f\"{key}: {correct / total * 100:.2f}%\")\n",
    "\n",
    "food_total_sum = 0\n",
    "food_correct_sum = 0\n",
    "\n",
    "print(\"---\")\n",
    "for key in food_evaluation:\n",
    "    correct = food_evaluation[key][\"correct\"]\n",
    "    total = food_evaluation[key][\"total\"]\n",
    "    \n",
    "    food_total_sum += total\n",
    "    food_correct_sum += correct\n",
    "\n",
    "    print(f\"{key}: {correct / total * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTotal: {food_correct_sum/food_total_sum * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c01abb",
   "metadata": {},
   "source": [
    "# Evaluating Existing Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2417dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary which will be populated with the entities and result information\n",
    "entity_evaluation = {}\n",
    "\n",
    "# helper function to udpate the entity_evaluation dictionary\n",
    "def update_results(entity, metric):\n",
    "    if entity not in entity_evaluation:\n",
    "        entity_evaluation[entity] = {\"correct\": 0, \"total\": 0}\n",
    "    \n",
    "    entity_evaluation[entity][metric] += 1\n",
    "\n",
    "# same as before, see if entities from test set match what spaCy currently predicts\n",
    "for data in TEST_REVISION_DATA:\n",
    "    sentence = data[0]\n",
    "    entities = data[1][\"entities\"]\n",
    "\n",
    "    for entity in entities:\n",
    "        doc = nlp(sentence)\n",
    "        correct_text = sentence[entity[0]:entity[1]]\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == entity[2] and ent.text == correct_text:\n",
    "                update_results(ent.label_, \"correct\")\n",
    "                break\n",
    "\n",
    "        update_results(entity[2], \"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c87295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON | 86.70%\n",
      "ORG | 53.74%\n",
      "GPE | 87.51%\n",
      "WORK_OF_ART | 62.62%\n",
      "DATE | 78.00%\n",
      "ORDINAL | 97.63%\n",
      "FAC | 67.33%\n",
      "TIME | 79.60%\n",
      "CARDINAL | 77.80%\n",
      "NORP | 91.09%\n",
      "LOC | 78.15%\n",
      "PRODUCT | 58.40%\n",
      "MONEY | 90.00%\n",
      "QUANTITY | 83.87%\n",
      "PERCENT | 94.46%\n",
      "EVENT | 67.65%\n",
      "LANGUAGE | 93.75%\n",
      "LAW | 60.00%\n",
      "\n",
      "Overall accuracy: 78.17%\n"
     ]
    }
   ],
   "source": [
    "sum_total = 0\n",
    "sum_correct = 0\n",
    "\n",
    "for entity in entity_evaluation:\n",
    "    total = entity_evaluation[entity][\"total\"]\n",
    "    correct = entity_evaluation[entity][\"correct\"]\n",
    "\n",
    "    sum_total += total\n",
    "    sum_correct += correct\n",
    "    \n",
    "    print(\"{} | {:.2f}%\".format(entity, correct / total * 100))\n",
    "\n",
    "print()\n",
    "print(\"Overall accuracy: {:.2f}%\".format(sum_correct / sum_total * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d397724",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38bd3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.meta[\"name\"] = \"food_entity_extractor_v2\"\n",
    "nlp.to_disk(\"./models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
